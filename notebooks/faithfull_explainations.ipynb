{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from ettcl.core.pipelines import ClassificationPipeline\n",
    "\n",
    "dataset_path = \"/home/IAIS/hiser/data/trec-6/\"\n",
    "checkpoint = \"/home/IAIS/hiser/dev/hyped/examples/output/trec_6_bert/model/best-model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(dataset_path)\n",
    "dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions from different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_pipeline = ClassificationPipeline(\n",
    "    model_name_or_path=checkpoint,\n",
    "    architecture='bert',\n",
    ")\n",
    "baseline_preds = baseline_pipeline.predict(dataset['test']['text'])\n",
    "baseline_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/IAIS/hiser/dev/hyped/examples/output/trec_6_bert/model/best-model/. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/IAIS/hiser/dev/hyped/examples/output/trec_6_bert/model/best-model/ were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3cf3316fb946ffbec06507a766c3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding (num_proc=2):   0%|          | 0/4906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffaafeda74184917917afe63afef707a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71e433a4904404d96943c440cb7b141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Searching (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/IAIS/hiser/miniconda3/envs/colbert/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_pipeline = ClassificationPipeline(\n",
    "    model_name_or_path=checkpoint,\n",
    "    architecture='sbert',\n",
    ")\n",
    "\n",
    "sbert_pipeline.train_index(dataset['train'])\n",
    "\n",
    "sbert_preds = sbert_pipeline.predict(dataset['test']['text'])\n",
    "sbert_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/IAIS/hiser/dev/hyped/examples/output/trec_6_bert/model/best-model/ were not used when initializing ColBERTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing ColBERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ColBERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "#> Will delete 14 files already at outputs/index_colbert\n",
      "[0][2023-07-17 17:22:59,880] [INFO] [init] nranks = 2 \t gpus = [0, 1] \t device=0\n",
      "[1][2023-07-17 17:23:04,379] [INFO] [init] nranks = 2 \t gpus = [0, 1] \t device=1\n",
      "[0][2023-07-17 17:23:05,427] [INFO] [print_message] [0] \t\t # of sampled PIDs = 4906 \t sampled_pids[:3] = [3412, 83, 2446]\n",
      "Encoding: 100%|██████████| 10/10 [00:01<00:00,  8.92it/s]\n",
      "Encoding: 100%|██████████| 10/10 [00:01<00:00,  8.21it/s]\n",
      "[0][2023-07-17 17:23:07,448] [INFO] [print_message] [0] \t\t avg_doclen_est = 14.464339256286621 \t len(local_sample) = 2,454\n",
      "[1][2023-07-17 17:23:07,448] [INFO] [print_message] [1] \t\t avg_doclen_est = 14.464339256286621 \t len(local_sample) = 2,452\n",
      "[0][2023-07-17 17:23:07,649] [INFO] [print_message] [0] \t\t Creaing 4,096 partitions.\n",
      "[0][2023-07-17 17:23:07,649] [INFO] [print_message] [0] \t\t *Estimated* 70,962 embeddings.\n",
      "[0][2023-07-17 17:23:07,649] [INFO] [print_message] [0] \t\t #> Saving the indexing plan to outputs/index_colbert/plan.json ..\n",
      "WARNING clustering 67414 points to 4096 centroids: please provide at least 159744 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 67414 points in 768D to 4096 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.05 s\n",
      "  Iteration 3 (0.30 s, search 0.25 s): objective=6509.11 imbalance=1.665 nsplit=0       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0][2023-07-17 17:23:10,748] [INFO] [print_message] Loading decompress_residuals_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0][2023-07-17 17:23:10,891] [INFO] [print_message] Loading packbits_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0][2023-07-17 17:23:11,045] [INFO] [print_message] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[0][2023-07-17 17:23:11,046] [INFO] [print_message] #> Got bucket_cutoffs = tensor([-4.7607e-03,  3.0518e-05,  4.8370e-03], device='cuda:0') and bucket_weights = tensor([-0.0104, -0.0020,  0.0020,  0.0106], device='cuda:0')\n",
      "[0][2023-07-17 17:23:11,046] [INFO] [print_message] avg_residual = 0.00821685791015625\n",
      "[1][2023-07-17 17:23:11,096] [INFO] [print_message] Loading decompress_residuals_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "0it [00:00, ?it/s]\n",
      "Encoding:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A[1][2023-07-17 17:23:11,212] [INFO] [print_message] Loading packbits_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "\n",
      "Encoding:  20%|██        | 2/10 [00:00<00:00, 17.62it/s]\u001b[A\n",
      "Encoding:   0%|          | 0/10 [00:00<?, ?it/s].95it/s]\u001b[A\n",
      "Encoding:  40%|████      | 4/10 [00:00<00:00, 15.28it/s]\u001b[A\n",
      "Encoding:  60%|██████    | 6/10 [00:00<00:00, 13.23it/s]\u001b[A\n",
      "Encoding: 100%|██████████| 10/10 [00:00<00:00, 12.48it/s]\u001b[A\n",
      "[0][2023-07-17 17:23:12,089] [INFO] [print_message] [0] \t\t #> Saving chunk 0: \t 2,454 passages and 35,434 embeddings. From #0 onward.\n",
      "1it [00:01,  1.05s/it]█  | 8/10 [00:00<00:00, 12.05it/s]\n",
      "Encoding: 100%|██████████| 10/10 [00:00<00:00, 11.71it/s]\n",
      "[0][2023-07-17 17:23:12,469] [INFO] [print_message] [0] \t\t #> Checking all files were saved...\n",
      "[0][2023-07-17 17:23:12,469] [INFO] [print_message] [0] \t\t Found all files!\n",
      "[0][2023-07-17 17:23:12,475] [INFO] [print_message] [0] \t\t #> Building IVF...\n",
      "[0][2023-07-17 17:23:12,475] [INFO] [print_message] [0] \t\t #> Loading codes...\n",
      "100%|██████████| 2/2 [00:00<00:00, 787.15it/s]\n",
      "[0][2023-07-17 17:23:12,478] [INFO] [print_message] [0] \t\t Sorting codes...\n",
      "[0][2023-07-17 17:23:12,483] [INFO] [print_message] [0] \t\t Getting unique codes...\n",
      "[0][2023-07-17 17:23:12,483] [INFO] [print_message] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[0][2023-07-17 17:23:12,483] [INFO] [print_message] #> Building the emb2pid mapping..\n",
      "[0][2023-07-17 17:23:12,498] [INFO] [print_message] len(emb2pid) = 70962\n",
      "[0][2023-07-17 17:23:12,528] [INFO] [print_message] #> Saved optimized IVF to outputs/index_colbert/ivf.pid.pt\n",
      "[0][2023-07-17 17:23:12,529] [INFO] [print_message] [0] \t\t #> Saving the indexing metadata to outputs/index_colbert/metadata.json ..\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9355d7e52af486aa6b61db2e1612c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Searching (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colbert_pipeline = ClassificationPipeline(\n",
    "    model_name_or_path=checkpoint,\n",
    "    architecture='colbert',\n",
    ")\n",
    "\n",
    "colbert_pipeline.train_index(dataset['train'])\n",
    "\n",
    "colbert_preds = colbert_pipeline.predict(dataset['test']['text'])\n",
    "colbert_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/IAIS/hiser/dev/hyped/examples/output/trec_6_bert/model/best-model/ were not used when initializing SentenceColBERTModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing SentenceColBERTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SentenceColBERTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Loading cached processed dataset at /home/IAIS/hiser/data/trec-6/train/cache-a45ab04b1b89767f.arrow\n",
      "#> Will delete 14 files already at outputs/index_scolbert\n",
      "[0][2023-07-17 17:23:35,026] [INFO] [init] nranks = 2 \t gpus = [0, 1] \t device=0\n",
      "[1][2023-07-17 17:23:39,737] [INFO] [init] nranks = 2 \t gpus = [0, 1] \t device=1\n",
      "[0][2023-07-17 17:23:40,751] [INFO] [print_message] [0] \t\t # of sampled PIDs = 4906 \t sampled_pids[:3] = [3412, 83, 2446]\n",
      "Encoding: 100%|██████████| 10/10 [00:01<00:00,  5.05it/s]\n",
      "Encoding: 100%|██████████| 10/10 [00:01<00:00,  5.40it/s]\n",
      "[0][2023-07-17 17:23:43,458] [INFO] [print_message] [0] \t\t avg_doclen_est = 1.0148802995681763 \t len(local_sample) = 2,454\n",
      "[1][2023-07-17 17:23:43,459] [INFO] [print_message] [1] \t\t avg_doclen_est = 1.0148802995681763 \t len(local_sample) = 2,452\n",
      "[0][2023-07-17 17:23:43,475] [INFO] [print_message] [0] \t\t Creaing 1,024 partitions.\n",
      "[0][2023-07-17 17:23:43,476] [INFO] [print_message] [0] \t\t *Estimated* 4,979 embeddings.\n",
      "[0][2023-07-17 17:23:43,476] [INFO] [print_message] [0] \t\t #> Saving the indexing plan to outputs/index_scolbert/plan.json ..\n",
      "WARNING clustering 4731 points to 1024 centroids: please provide at least 39936 training points\n",
      "[0][2023-07-17 17:23:46,009] [INFO] [print_message] Loading decompress_residuals_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering 4731 points in 768D to 1024 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 3 (0.03 s, search 0.02 s): objective=197.702 imbalance=2.063 nsplit=0        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0][2023-07-17 17:23:46,180] [INFO] [print_message] Loading packbits_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0][2023-07-17 17:23:46,380] [INFO] [print_message] #> Got bucket_cutoffs_quantiles = tensor([0.2500, 0.5000, 0.7500], device='cuda:0') and bucket_weights_quantiles = tensor([0.1250, 0.3750, 0.6250, 0.8750], device='cuda:0')\n",
      "[0][2023-07-17 17:23:46,382] [INFO] [print_message] #> Got bucket_cutoffs = tensor([-0.0047,  0.0000,  0.0048], device='cuda:0') and bucket_weights = tensor([-0.0091, -0.0020,  0.0021,  0.0091], device='cuda:0')\n",
      "[0][2023-07-17 17:23:46,382] [INFO] [print_message] avg_residual = 0.006805419921875\n",
      "[1][2023-07-17 17:23:46,411] [INFO] [print_message] Loading decompress_residuals_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "0it [00:00, ?it/s][1][2023-07-17 17:23:46,585] [INFO] [print_message] Loading packbits_cuda_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "\n",
      "Encoding:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Encoding:  10%|█         | 1/10 [00:00<00:01,  6.60it/s]\u001b[A\n",
      "Encoding:   0%|          | 0/10 [00:00<?, ?it/s].62it/s]\u001b[A\n",
      "Encoding:  10%|█         | 1/10 [00:00<00:01,  6.68it/s]\u001b[A\n",
      "Encoding:  20%|██        | 2/10 [00:00<00:01,  6.49it/s]\u001b[A\n",
      "Encoding:  30%|███       | 3/10 [00:00<00:01,  6.49it/s]\u001b[A\n",
      "Encoding:  40%|████      | 4/10 [00:00<00:00,  6.58it/s]\u001b[A\n",
      "Encoding:  50%|█████     | 5/10 [00:00<00:00,  6.64it/s]\u001b[A\n",
      "Encoding:  60%|██████    | 6/10 [00:00<00:00,  6.69it/s]\u001b[A\n",
      "Encoding:  80%|████████  | 8/10 [00:01<00:00,  6.71it/s]\u001b[A\n",
      "Encoding: 100%|██████████| 10/10 [00:01<00:00,  6.30it/s]\u001b[A\n",
      "[0][2023-07-17 17:23:48,258] [INFO] [print_message] [0] \t\t #> Saving chunk 0: \t 2,454 passages and 2,487 embeddings. From #0 onward.\n",
      "1it [00:01,  1.85s/it]\n",
      "Encoding: 100%|██████████| 10/10 [00:01<00:00,  5.84it/s]\n",
      "[0][2023-07-17 17:23:48,769] [INFO] [print_message] [0] \t\t #> Checking all files were saved...\n",
      "[0][2023-07-17 17:23:48,769] [INFO] [print_message] [0] \t\t Found all files!\n",
      "[0][2023-07-17 17:23:48,777] [INFO] [print_message] [0] \t\t #> Building IVF...\n",
      "[0][2023-07-17 17:23:48,777] [INFO] [print_message] [0] \t\t #> Loading codes...\n",
      "100%|██████████| 2/2 [00:00<00:00, 658.76it/s]\n",
      "[0][2023-07-17 17:23:48,780] [INFO] [print_message] [0] \t\t Sorting codes...\n",
      "[0][2023-07-17 17:23:48,781] [INFO] [print_message] [0] \t\t Getting unique codes...\n",
      "[0][2023-07-17 17:23:48,781] [INFO] [print_message] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[0][2023-07-17 17:23:48,781] [INFO] [print_message] #> Building the emb2pid mapping..\n",
      "[0][2023-07-17 17:23:48,794] [INFO] [print_message] len(emb2pid) = 4979\n",
      "[0][2023-07-17 17:23:48,805] [INFO] [print_message] #> Saved optimized IVF to outputs/index_scolbert/ivf.pid.pt\n",
      "[0][2023-07-17 17:23:48,805] [INFO] [print_message] [0] \t\t #> Saving the indexing metadata to outputs/index_scolbert/metadata.json ..\n",
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe4ea5534544dee841fa0084ede6e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Searching (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([500])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scolbert_pipeline = ClassificationPipeline(\n",
    "    model_name_or_path=checkpoint,\n",
    "    architecture='scolbert',\n",
    "    tokenizer_kwargs={'query_maxlen': 32, 'doc_maxlen': 32}\n",
    ")\n",
    "\n",
    "scolbert_pipeline.train_index(dataset['train'])\n",
    "\n",
    "scolbert_preds = scolbert_pipeline.predict(dataset['test']['text'])\n",
    "scolbert_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(y_true, y_pred, t_mask, f_mask):\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=0)\n",
    "    print(\"Whole dataset\")\n",
    "    print(f\"\\t precision: {p:.3f}\")\n",
    "    print(f\"\\t recall: {p:.3f}\")\n",
    "    print(f\"\\t f1: {f:.3f}\")\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true[t_mask], y_pred[t_mask], average='micro', zero_division=0)\n",
    "    print(\"True predictions\")\n",
    "    print(f\"\\t precision: {p:.3f}\")\n",
    "    print(f\"\\t recall: {p:.3f}\")\n",
    "    print(f\"\\t f1: {f:.3f}\")\n",
    "\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true[f_mask], y_pred[f_mask], average='micro', zero_division=0)\n",
    "    print(\"False predictions\")\n",
    "    print(f\"\\t precision: {p:.3f}\")\n",
    "    print(f\"\\t recall: {p:.3f}\")\n",
    "    print(f\"\\t f1: {f:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support: None\n",
      "precision: 0.978\n",
      "recall: 0.978\n",
      "f1: 0.978\n"
     ]
    }
   ],
   "source": [
    "t_mask = baseline_preds == dataset['test']['label']\n",
    "f_mask = baseline_preds != dataset['test']['label']\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support(dataset['test']['label'], baseline_preds, average='micro', zero_division=0)\n",
    "print(\"support:\", s)\n",
    "print(\"precision:\", p)\n",
    "print(\"recall:\", r)\n",
    "print(\"f1:\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset\n",
      "\t precision: 1.000\n",
      "\t recall: 1.000\n",
      "\t f1: 1.000\n",
      "True predictions\n",
      "\t precision: 1.000\n",
      "\t recall: 1.000\n",
      "\t f1: 1.000\n",
      "False predictions\n",
      "\t precision: 1.000\n",
      "\t recall: 1.000\n",
      "\t f1: 1.000\n"
     ]
    }
   ],
   "source": [
    "evaluate_faithfulness(baseline_preds, sbert_preds, t_mask, f_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset\n",
      "\t precision: 0.982\n",
      "\t recall: 0.982\n",
      "\t f1: 0.982\n",
      "True predictions\n",
      "\t precision: 0.986\n",
      "\t recall: 0.986\n",
      "\t f1: 0.986\n",
      "False predictions\n",
      "\t precision: 0.818\n",
      "\t recall: 0.818\n",
      "\t f1: 0.818\n"
     ]
    }
   ],
   "source": [
    "evaluate_faithfulness(baseline_preds, colbert_preds, t_mask, f_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole dataset\n",
      "\t precision: 0.748\n",
      "\t recall: 0.748\n",
      "\t f1: 0.748\n",
      "True predictions\n",
      "\t precision: 0.751\n",
      "\t recall: 0.751\n",
      "\t f1: 0.751\n",
      "False predictions\n",
      "\t precision: 0.636\n",
      "\t recall: 0.636\n",
      "\t f1: 0.636\n"
     ]
    }
   ],
   "source": [
    "evaluate_faithfulness(baseline_preds, scolbert_preds, t_mask, f_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example explainations from kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_dataset_infos\n",
    "\n",
    "info = get_dataset_infos('trec')\n",
    "id2lbl = {i: l for i, l in enumerate(dataset['test'].features['label'].names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[118, 135, 190, 223, 249, 251, 291, 365, 425, 460, 488]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = colbert_pipeline\n",
    "(baseline_preds != dataset['test']['label']).nonzero().squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_explaination(idx: int, match_idx: int):\n",
    "    print(f\"GT Label: {id2lbl[dataset['test'][idx]['label'].item()]}\")\n",
    "    print(f\"Baseline Label: {id2lbl[baseline_preds[idx].item()]}\")\n",
    "    print(dataset['test'][idx]['text'])\n",
    "    print()\n",
    "\n",
    "    match_pids = pipeline.search_result['match_pids'][idx]\n",
    "    matches = dataset['train'].select(match_pids)\n",
    "\n",
    "    print(f\"Label: {id2lbl[matches[match_idx]['label'].item()]}\")\n",
    "    print(matches[match_idx]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Label: NUM\n",
      "Baseline Label: ENTY\n",
      "What was the last year that the Chicago Cubs won the World Series ?\n",
      "\n",
      "Label: NUM\n",
      "In what year was the movie the Ten Commandments released ?\n"
     ]
    }
   ],
   "source": [
    "print_explaination(idx=118, match_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Label: ENTY\n",
      "Baseline Label: LOC\n",
      "What is the major fault line near Kentucky ?\n",
      "\n",
      "Label: LOC\n",
      "What do we call the imaginary line along the top of the Rocky Mountains ?\n"
     ]
    }
   ],
   "source": [
    "print_explaination(idx=135, match_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT Label: ENTY\n",
      "Baseline Label: DESC\n",
      "What is the sales tax in Minnesota ?\n",
      "\n",
      "Label: DESC\n",
      "What according to the Kinsey Institute , is the sexual preference of four percent of American males ?\n"
     ]
    }
   ],
   "source": [
    "print_explaination(idx=190, match_idx=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
