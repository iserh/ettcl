{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ettcl.encoding import ColBERTEncoder\n",
    "from ettcl.utils.utils import split_into_sentences\n",
    "from ettcl.modeling.tokenization_sentence_colbert import SentenceTokenizer\n",
    "from ettcl.modeling.modeling_sentence_colbert import SentenceColBERTForReranking\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/IAIS/hiser/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "Found cached dataset imdb (/home/IAIS/hiser/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "test_dataset = load_dataset(\"imdb\", split=\"test\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/IAIS/hiser/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-ffa864a77e2b118e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04da9c890694aa7a76c47fa0c112c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'sents'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda text: {\"sents\": split_into_sentences(text)},\n",
    "    input_columns=\"text\",\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    lambda text: {\"sents\": split_into_sentences(text)},\n",
    "    input_columns=\"text\",\n",
    ")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing SentenceColBERTForReranking: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing SentenceColBERTForReranking from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SentenceColBERTForReranking from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentenceColBERTForReranking.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = SentenceTokenizer.from_pretrained(\"bert-base-uncased\", doc_maxlen=32, query_maxlen=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check triple processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ettcl.core.triple_sampling import DataCollatorForTriples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd39f130020b4535bc52134f2752ca03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'sents', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda sents: tokenizer(sents, truncation=True),\n",
    "    input_columns=\"sents\",\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples = [\n",
    "    [\n",
    "        train_dataset.select_columns([\"input_ids\", \"attention_mask\"])[j]\n",
    "        for j in range(i, i+4)\n",
    "    ]\n",
    "    for i in range(0, 400, 4)\n",
    "]\n",
    "len(triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 8\n",
      "nway: 4\n",
      "num_sentences: 16\n",
      "sentence_length: 32\n",
      "torch.Size([8, 4, 16, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.1147], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTriples(tokenizer)\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    triples,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "batch = next(iter(dl))\n",
    "\n",
    "print(\"batch_size:\", len(batch[\"input_ids\"]))\n",
    "print(\"nway:\", len(batch[\"input_ids\"][0]))\n",
    "print(\"num_sentences:\", len(batch[\"input_ids\"][0][0]))\n",
    "print(\"sentence_length:\", len(batch[\"input_ids\"][0][0][0]))\n",
    "print(batch[\"input_ids\"].shape)\n",
    "\n",
    "model(**batch).loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Indexing & Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ettcl.indexing import ColBERTIndexer\n",
    "from ettcl.searching import ColBERTSearcher\n",
    "from ettcl.utils.multiprocessing import run_multiprocessed\n",
    "\n",
    "index_path = \"models/imdb/sentence_colbert/index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ColBERTEncoder(model.colbert, tokenizer)\n",
    "indexer = ColBERTIndexer(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer.index(index_path, train_dataset[\"sents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting TOKENIZERS_PARALLELISM=false for forked processes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a497b2f7404f4fecb7d8dd3730301a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'sents', 'match_pids', 'match_scores'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searcher = ColBERTSearcher(index_path, encoder)\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    run_multiprocessed(searcher.search),\n",
    "    input_columns=\"sents\",\n",
    "    fn_kwargs={\"k\": 50},\n",
    "    batched=True,\n",
    "    num_proc=2,\n",
    "    with_rank=True\n",
    ")\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/accuracy/1': 0.70936,\n",
       " '/precision/micro/1': 0.70936,\n",
       " '/precision/macro/1': 0.7096982066359365,\n",
       " '/recall/micro/1': 0.70936,\n",
       " '/recall/macro/1': 0.70936,\n",
       " '/f1/micro/1': 0.70936,\n",
       " '/f1/macro/1': 0.7092427648219298,\n",
       " '/accuracy/3': 0.74564,\n",
       " '/precision/micro/3': 0.74564,\n",
       " '/precision/macro/3': 0.74645996797576,\n",
       " '/recall/micro/3': 0.74564,\n",
       " '/recall/macro/3': 0.74564,\n",
       " '/f1/micro/3': 0.7456399999999999,\n",
       " '/f1/macro/3': 0.7454282610762658,\n",
       " '/accuracy/5': 0.764,\n",
       " '/precision/micro/5': 0.764,\n",
       " '/precision/macro/5': 0.7656060180232452,\n",
       " '/recall/micro/5': 0.764,\n",
       " '/recall/macro/5': 0.764,\n",
       " '/f1/micro/5': 0.764,\n",
       " '/f1/macro/5': 0.7636427094617858,\n",
       " '/accuracy/10': 0.77416,\n",
       " '/precision/micro/10': 0.77416,\n",
       " '/precision/macro/10': 0.7855452910860196,\n",
       " '/recall/micro/10': 0.77416,\n",
       " '/recall/macro/10': 0.77416,\n",
       " '/f1/micro/10': 0.77416,\n",
       " '/f1/macro/10': 0.771886155356883}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"\"\n",
    "ks = [1,3,5,10]\n",
    "\n",
    "match_pids = test_dataset[\"match_pids\"]\n",
    "if isinstance(match_pids, list):\n",
    "    print(\"WARNING\")\n",
    "    match_pids = torch.nn.utils.rnn.pad_sequence(match_pids, batch_first=True, padding_value=-1)\n",
    "\n",
    "match_labels = train_dataset[\"label\"][match_pids.tolist()]\n",
    "\n",
    "metrics = {}\n",
    "for k in ks:\n",
    "    knn = match_labels[:, :k]\n",
    "    y_pred = torch.mode(knn)[0]\n",
    "    assert -1 not in y_pred, \"Not enough matches\"\n",
    "\n",
    "    metrics[f\"{prefix}/accuracy/{k}\"] = accuracy_score(y_pred=y_pred, y_true=test_dataset[\"label\"])\n",
    "    metrics[f\"{prefix}/precision/micro/{k}\"] = precision_score(\n",
    "        y_pred=y_pred, y_true=test_dataset[\"label\"], average=\"micro\"\n",
    "    )\n",
    "    metrics[f\"{prefix}/precision/macro/{k}\"] = precision_score(\n",
    "        y_pred=y_pred, y_true=test_dataset[\"label\"], average=\"macro\"\n",
    "    )\n",
    "    metrics[f\"{prefix}/recall/micro/{k}\"] = recall_score(\n",
    "        y_pred=y_pred, y_true=test_dataset[\"label\"], average=\"micro\"\n",
    "    )\n",
    "    metrics[f\"{prefix}/recall/macro/{k}\"] = recall_score(\n",
    "        y_pred=y_pred, y_true=test_dataset[\"label\"], average=\"macro\"\n",
    "    )\n",
    "    metrics[f\"{prefix}/f1/micro/{k}\"] = f1_score(\n",
    "        y_pred=y_pred, y_true=test_dataset[\"label\"], average=\"micro\"\n",
    "    )\n",
    "    metrics[f\"{prefix}/f1/macro/{k}\"] = f1_score(\n",
    "        y_pred=y_pred, y_true=test_dataset[\"label\"], average=\"macro\"\n",
    "    )\n",
    "\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
